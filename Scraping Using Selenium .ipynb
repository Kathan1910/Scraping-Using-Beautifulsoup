{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1989f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Partner Name\tLevel\tLocation\tWebUrls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f635c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of links scraped: 244\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize the Chrome WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "driver.get('https://www.liferay.com/partners/directory?')\n",
    "\n",
    "# Explicitly wait for the element with class 'element-border' to be present\n",
    "wait = WebDriverWait(driver, 30)  # Wait up to 30 seconds\n",
    "wait.until(EC.presence_of_all_elements_located((By.CLASS_NAME, \"element-border\")))\n",
    "\n",
    "# Execute JavaScript to extract href links from a tags with class 'element-border'\n",
    "href_links = driver.execute_script('''\n",
    "    const links = [];\n",
    "    const elements = document.querySelectorAll('a.element-border');\n",
    "    elements.forEach(function(element) {\n",
    "        const href = element.getAttribute('href');\n",
    "        if (href) {\n",
    "            links.push(href);\n",
    "        }\n",
    "    });\n",
    "    return links;\n",
    "''')\n",
    "\n",
    "# Print the number of scraped links\n",
    "print(f\"Total number of links scraped: {len(href_links)}\")\n",
    "\n",
    "# Save the links to a CSV file\n",
    "df = pd.DataFrame(href_links, columns=[\"Links\"])\n",
    "df.to_csv(\"scraped_links.csv\", index=False)\n",
    "\n",
    "# Pause for 10 seconds\n",
    "time.sleep(10)\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "366184bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of links scraped: 244\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize the Chrome WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "driver.get('https://www.liferay.com/partners/directory?')\n",
    "\n",
    "# Explicitly wait for the element with class 'element-border' to be present\n",
    "wait = WebDriverWait(driver, 30)  # Wait up to 30 seconds\n",
    "wait.until(EC.presence_of_all_elements_located((By.CLASS_NAME, \"element-border\")))\n",
    "\n",
    "# Execute JavaScript to extract href links from a tags with class 'element-border'\n",
    "href_links = driver.execute_script('''\n",
    "    const links = [];\n",
    "    const elements = document.querySelectorAll('a.element-border');\n",
    "    elements.forEach(function(element) {\n",
    "        const href = element.getAttribute('href');\n",
    "        if (href) {\n",
    "            links.push(href);\n",
    "        }\n",
    "    });\n",
    "    return links;\n",
    "''')\n",
    "\n",
    "# Print the number of scraped links\n",
    "print(f\"Total number of links scraped: {len(href_links)}\")\n",
    "\n",
    "# Save the links to a CSV file\n",
    "df = pd.DataFrame(href_links, columns=[\"Links\"])\n",
    "df.to_csv(\"scraped_links2.csv\", index=False)\n",
    "\n",
    "# Pause for 10 seconds\n",
    "time.sleep(10)\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af7cc9f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "TimeoutException",
     "evalue": "Message: timeout: Timed out receiving message from renderer: 299.561\n  (Session info: chrome=117.0.5938.132)\nStacktrace:\n\tGetHandleVerifier [0x00007FF7EFCB7892+54818]\n\t(No symbol) [0x00007FF7EFC26AC2]\n\t(No symbol) [0x00007FF7EFADDA3B]\n\t(No symbol) [0x00007FF7EFACC06A]\n\t(No symbol) [0x00007FF7EFACBFB3]\n\t(No symbol) [0x00007FF7EFACA82E]\n\t(No symbol) [0x00007FF7EFACB2EE]\n\t(No symbol) [0x00007FF7EFAD775B]\n\t(No symbol) [0x00007FF7EFAE84D1]\n\t(No symbol) [0x00007FF7EFAEC80A]\n\t(No symbol) [0x00007FF7EFACB937]\n\t(No symbol) [0x00007FF7EFAE83AA]\n\t(No symbol) [0x00007FF7EFB57609]\n\t(No symbol) [0x00007FF7EFB3E883]\n\t(No symbol) [0x00007FF7EFB13691]\n\t(No symbol) [0x00007FF7EFB148D4]\n\tGetHandleVerifier [0x00007FF7F001B992+3610402]\n\tGetHandleVerifier [0x00007FF7F0071860+3962352]\n\tGetHandleVerifier [0x00007FF7F0069D4F+3930847]\n\tGetHandleVerifier [0x00007FF7EFD53646+693206]\n\t(No symbol) [0x00007FF7EFC31628]\n\t(No symbol) [0x00007FF7EFC2D934]\n\t(No symbol) [0x00007FF7EFC2DA62]\n\t(No symbol) [0x00007FF7EFC1E113]\n\tBaseThreadInitThunk [0x00007FFDCB7A7344+20]\n\tRtlUserThreadStart [0x00007FFDCCA626B1+33]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTimeoutException\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10332/2827367651.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;31m# Loop through each link and scrape details\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mlink\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhref_links\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m     \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlink\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m    351\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m         \u001b[1;34m\"\"\"Loads a web page in the current browser session.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 353\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCommand\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGET\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"url\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    342\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    343\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 344\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    345\u001b[0m             \u001b[0mresponse\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"value\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_unwrap_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"value\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    346\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py\u001b[0m in \u001b[0;36mcheck_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    227\u001b[0m                 \u001b[0malert_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"alert\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"text\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malert_text\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 229\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTimeoutException\u001b[0m: Message: timeout: Timed out receiving message from renderer: 299.561\n  (Session info: chrome=117.0.5938.132)\nStacktrace:\n\tGetHandleVerifier [0x00007FF7EFCB7892+54818]\n\t(No symbol) [0x00007FF7EFC26AC2]\n\t(No symbol) [0x00007FF7EFADDA3B]\n\t(No symbol) [0x00007FF7EFACC06A]\n\t(No symbol) [0x00007FF7EFACBFB3]\n\t(No symbol) [0x00007FF7EFACA82E]\n\t(No symbol) [0x00007FF7EFACB2EE]\n\t(No symbol) [0x00007FF7EFAD775B]\n\t(No symbol) [0x00007FF7EFAE84D1]\n\t(No symbol) [0x00007FF7EFAEC80A]\n\t(No symbol) [0x00007FF7EFACB937]\n\t(No symbol) [0x00007FF7EFAE83AA]\n\t(No symbol) [0x00007FF7EFB57609]\n\t(No symbol) [0x00007FF7EFB3E883]\n\t(No symbol) [0x00007FF7EFB13691]\n\t(No symbol) [0x00007FF7EFB148D4]\n\tGetHandleVerifier [0x00007FF7F001B992+3610402]\n\tGetHandleVerifier [0x00007FF7F0071860+3962352]\n\tGetHandleVerifier [0x00007FF7F0069D4F+3930847]\n\tGetHandleVerifier [0x00007FF7EFD53646+693206]\n\t(No symbol) [0x00007FF7EFC31628]\n\t(No symbol) [0x00007FF7EFC2D934]\n\t(No symbol) [0x00007FF7EFC2DA62]\n\t(No symbol) [0x00007FF7EFC1E113]\n\tBaseThreadInitThunk [0x00007FFDCB7A7344+20]\n\tRtlUserThreadStart [0x00007FFDCCA626B1+33]\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize the Chrome WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "driver.get('https://www.liferay.com/partners/directory?')\n",
    "\n",
    "# Explicitly wait for the element with class 'element-border' to be present\n",
    "wait = WebDriverWait(driver, 30)\n",
    "wait.until(EC.presence_of_all_elements_located((By.CLASS_NAME, \"element-border\")))\n",
    "\n",
    "# Extract href links from a tags with class 'element-border'\n",
    "href_links = driver.execute_script('''\n",
    "    const links = [];\n",
    "    const elements = document.querySelectorAll('a.element-border');\n",
    "    elements.forEach(function(element) {\n",
    "        const href = element.getAttribute('href');\n",
    "        if (href) {\n",
    "            links.push(href);\n",
    "        }\n",
    "    });\n",
    "    return links;\n",
    "''')\n",
    "\n",
    "partners_data = []\n",
    "\n",
    "# Loop through each link and scrape details\n",
    "for link in href_links:\n",
    "    driver.get(link)\n",
    "    \n",
    "    try:\n",
    "        name = driver.find_element_by_css_selector(\"h1.company-name\").text\n",
    "    except:\n",
    "        name = \"Not Available\"\n",
    "    \n",
    "    try:\n",
    "        location = driver.find_element_by_css_selector(\"div.country.small-caps\").text\n",
    "    except:\n",
    "        location = \"Not Available\"\n",
    "\n",
    "    try:\n",
    "        url = driver.find_element_by_css_selector('a[rel=\"noopener\"]').get_attribute(\"href\")\n",
    "    except:\n",
    "        url = \"Not Available\"\n",
    "    \n",
    "    try:\n",
    "        level = driver.find_element_by_css_selector(\"p.level\").text\n",
    "    except:\n",
    "        level = \"Not Available\"\n",
    "    \n",
    "    partners_data.append({\n",
    "        'Partner Name': name,\n",
    "        'Partner Location': location,\n",
    "        'Partner URL': url,\n",
    "        'Partner Level': level\n",
    "    })\n",
    "\n",
    "# Save the scraped data into a CSV file\n",
    "df = pd.DataFrame(partners_data)\n",
    "df.to_csv(\"partners_data.csv\", index=False)\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec9038fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Partner Name': 'ACCENTURE, LLP', 'Partner Location': 'Not Available', 'Partner URL': 'https://www.accenture.com/', 'Partner Level': 'Not Available'}\n",
      "{'Partner Name': 'CAPGEMINI US', 'Partner Location': 'Not Available', 'Partner URL': 'http://www.capgemini.com/', 'Partner Level': 'Not Available'}\n",
      "{'Partner Name': 'COGNIZANT TECHNOLOGY SOLUTIONS', 'Partner Location': 'Not Available', 'Partner URL': 'http://www.cognizant.com/', 'Partner Level': 'Not Available'}\n",
      "{'Partner Name': 'INFOSYS LIMITED', 'Partner Location': 'Not Available', 'Partner URL': 'https://www.infosys.com/', 'Partner Level': 'Not Available'}\n",
      "{'Partner Name': 'LTIMINDTREE', 'Partner Location': 'Not Available', 'Partner URL': 'https://www.ltimindtree.com/', 'Partner Level': 'Not Available'}\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "\n",
    "# Load the previously scraped CSV file\n",
    "df_links = pd.read_csv(\"scraped_links.csv\")\n",
    "urls = df_links[\"Links\"].head(5).tolist()\n",
    "\n",
    "# Initialize the Chrome WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "wait = WebDriverWait(driver, 7)  # Set the explicit wait time to 30 seconds\n",
    "\n",
    "partners_data = []\n",
    "\n",
    "# Loop through the first 5 links and scrape details\n",
    "for link in urls:\n",
    "    driver.get(link)\n",
    "\n",
    "    try:\n",
    "        name = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \"h1.company-name\"))).text\n",
    "    except:\n",
    "        name = \"Not Available\"\n",
    "\n",
    "    try:\n",
    "        location = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \"div.country.small-caps\"))).text\n",
    "    except:\n",
    "        location = \"Not Available\"\n",
    "\n",
    "    try:\n",
    "        url = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'a[rel=\"noopener\"]'))).get_attribute(\"href\")\n",
    "    except:\n",
    "        url = \"Not Available\"\n",
    "\n",
    "    try:\n",
    "        level = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \"p.level\"))).text\n",
    "    except:\n",
    "        level = \"Not Available\"\n",
    "\n",
    "    # Append scraped data to the list\n",
    "    partners_data.append({\n",
    "        'Partner Name': name,\n",
    "        'Partner Location': location,\n",
    "        'Partner URL': url,\n",
    "        'Partner Level': level\n",
    "    })\n",
    "\n",
    "# Print the scraped data for verification\n",
    "for data in partners_data:\n",
    "    print(data)\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ef910409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Partner Name': 'ACCENTURE, LLP', 'Partner Location': 'Not Available', 'Partner URL': 'https://www.accenture.com/', 'Partner Level': 'Not Available'}\n",
      "{'Partner Name': 'CAPGEMINI US', 'Partner Location': 'Not Available', 'Partner URL': 'http://www.capgemini.com/', 'Partner Level': 'Not Available'}\n",
      "{'Partner Name': 'COGNIZANT TECHNOLOGY SOLUTIONS', 'Partner Location': 'Not Available', 'Partner URL': 'http://www.cognizant.com/', 'Partner Level': 'Not Available'}\n",
      "{'Partner Name': 'INFOSYS LIMITED', 'Partner Location': 'Not Available', 'Partner URL': 'https://www.infosys.com/', 'Partner Level': 'Not Available'}\n",
      "{'Partner Name': 'LTIMINDTREE', 'Partner Location': 'Not Available', 'Partner URL': 'https://www.ltimindtree.com/', 'Partner Level': 'Not Available'}\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "\n",
    "# Load the previously scraped CSV file\n",
    "df_links = pd.read_csv(\"scraped_links.csv\")\n",
    "urls = df_links[\"Links\"].head(5).tolist()\n",
    "\n",
    "# Initialize the Chrome WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "wait = WebDriverWait(driver, 3)  # Set the explicit wait time to 30 seconds\n",
    "\n",
    "partners_data = []\n",
    "\n",
    "# Loop through the first 5 links and scrape details\n",
    "for link in urls:\n",
    "    driver.get(link)\n",
    "\n",
    "    try:\n",
    "        name = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \"h1.company-name\"))).text\n",
    "    except:\n",
    "        name = \"Not Available\"\n",
    "\n",
    "    try:\n",
    "        location = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \"div.corp-partnership-levels .country.small-caps\"))).text\n",
    "    except:\n",
    "        location = \"Not Available\"\n",
    "\n",
    "    try:\n",
    "        url = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'a[rel=\"noopener\"]'))).get_attribute(\"href\")\n",
    "    except:\n",
    "        url = \"Not Available\"\n",
    "\n",
    "    try:\n",
    "        level = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \"div.corp-partnership-levels p.level\"))).text\n",
    "    except:\n",
    "        level = \"Not Available\"\n",
    "\n",
    "    # Append scraped data to the list\n",
    "    partners_data.append({\n",
    "        'Partner Name': name,\n",
    "        'Partner Location': location,\n",
    "        'Partner URL': url,\n",
    "        'Partner Level': level\n",
    "    })\n",
    "\n",
    "# Print the scraped data for verification\n",
    "for data in partners_data:\n",
    "    print(data)\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c1a83133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Partner Name': ' Accenture, LLP ', 'Partner Location': 'Not Available', 'Partner URL': 'https://www.accenture.com', 'Partner Level': 'Not Available'}\n",
      "{'Partner Name': ' Capgemini US ', 'Partner Location': 'Not Available', 'Partner URL': 'http://www.capgemini.com', 'Partner Level': 'Not Available'}\n",
      "{'Partner Name': ' Cognizant Technology Solutions ', 'Partner Location': 'Not Available', 'Partner URL': 'http://www.cognizant.com/', 'Partner Level': 'Not Available'}\n",
      "{'Partner Name': ' Infosys Limited ', 'Partner Location': 'Not Available', 'Partner URL': 'https://www.infosys.com', 'Partner Level': 'Not Available'}\n",
      "{'Partner Name': ' LTIMindtree ', 'Partner Location': 'Not Available', 'Partner URL': 'https://www.ltimindtree.com', 'Partner Level': 'Not Available'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Load the previously scraped CSV file\n",
    "df_links = pd.read_csv(\"scraped_links.csv\")\n",
    "urls = df_links[\"Links\"].head(5).tolist()\n",
    "\n",
    "partners_data = []\n",
    "\n",
    "# Loop through the first 5 links and scrape details\n",
    "for link in urls:\n",
    "    response = requests.get(link)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    try:\n",
    "        name = soup.select_one(\"h1.company-name\").text\n",
    "    except:\n",
    "        name = \"Not Available\"\n",
    "\n",
    "    try:\n",
    "        location = soup.select_one(\"div.corp-partnership-levels .country.small-caps\").text\n",
    "    except:\n",
    "        location = \"Not Available\"\n",
    "\n",
    "    try:\n",
    "        url = soup.select_one('a[rel=\"noopener\"]').get(\"href\")\n",
    "    except:\n",
    "        url = \"Not Available\"\n",
    "\n",
    "    try:\n",
    "        level = soup.select_one(\"div.corp-partnership-levels p.level\").text\n",
    "    except:\n",
    "        level = \"Not Available\"\n",
    "\n",
    "    # Append scraped data to the list\n",
    "    partners_data.append({\n",
    "        'Partner Name': name,\n",
    "        'Partner Location': location,\n",
    "        'Partner URL': url,\n",
    "        'Partner Level': level\n",
    "    })\n",
    "\n",
    "# Print the scraped data for verification\n",
    "for data in partners_data:\n",
    "    print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "54b7feb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Partner Name': ' Accenture, LLP ', 'Partner Location': 'Not Available', 'Partner URL': 'https://www.accenture.com', 'Partner Level': 'Not Available'}\n",
      "{'Partner Name': ' Capgemini US ', 'Partner Location': 'Not Available', 'Partner URL': 'http://www.capgemini.com', 'Partner Level': 'Not Available'}\n",
      "{'Partner Name': ' Cognizant Technology Solutions ', 'Partner Location': 'Not Available', 'Partner URL': 'http://www.cognizant.com/', 'Partner Level': 'Not Available'}\n",
      "{'Partner Name': ' Infosys Limited ', 'Partner Location': 'Not Available', 'Partner URL': 'https://www.infosys.com', 'Partner Level': 'Not Available'}\n",
      "{'Partner Name': ' LTIMindtree ', 'Partner Location': 'Not Available', 'Partner URL': 'https://www.ltimindtree.com', 'Partner Level': 'Not Available'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Load the previously scraped CSV file\n",
    "df_links = pd.read_csv(\"scraped_links.csv\")\n",
    "urls = df_links[\"Links\"].head(5).tolist()\n",
    "\n",
    "partners_data = []\n",
    "\n",
    "# Loop through the first 5 links and scrape details\n",
    "for link in urls:\n",
    "    response = requests.get(link)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    try:\n",
    "        name = soup.select_one(\"h1.company-name\").text\n",
    "    except:\n",
    "        name = \"Not Available\"\n",
    "\n",
    "    try:\n",
    "        location = soup.select_one(\"div.country.small-caps\").text\n",
    "    except:\n",
    "        location = \"Not Available\"\n",
    "\n",
    "    try:\n",
    "        url = soup.select_one('a[rel=\"noopener\"]').get(\"href\")\n",
    "    except:\n",
    "        url = \"Not Available\"\n",
    "\n",
    "    try:\n",
    "        level = soup.select_one(\"p.level\").text\n",
    "    except:\n",
    "        level = \"Not Available\"\n",
    "\n",
    "    # Append scraped data to the list\n",
    "    partners_data.append({\n",
    "        'Partner Name': name,\n",
    "        'Partner Location': location,\n",
    "        'Partner URL': url,\n",
    "        'Partner Level': level\n",
    "    })\n",
    "\n",
    "# Print the scraped data for verification\n",
    "for data in partners_data:\n",
    "    print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "34ca575b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Partner Name': ' Accenture, LLP ', 'Partner Location': 'Not Available', 'Partner URL': 'https://www.accenture.com', 'Partner Level': 'Not Available'}\n",
      "{'Partner Name': ' Capgemini US ', 'Partner Location': 'Not Available', 'Partner URL': 'http://www.capgemini.com', 'Partner Level': 'Not Available'}\n",
      "{'Partner Name': ' Cognizant Technology Solutions ', 'Partner Location': 'Not Available', 'Partner URL': 'http://www.cognizant.com/', 'Partner Level': 'Not Available'}\n",
      "{'Partner Name': ' Infosys Limited ', 'Partner Location': 'Not Available', 'Partner URL': 'https://www.infosys.com', 'Partner Level': 'Not Available'}\n",
      "{'Partner Name': ' LTIMindtree ', 'Partner Location': 'Not Available', 'Partner URL': 'https://www.ltimindtree.com', 'Partner Level': 'Not Available'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Load the previously scraped CSV file\n",
    "df_links = pd.read_csv(\"scraped_links.csv\")\n",
    "urls = df_links[\"Links\"].head(5).tolist()\n",
    "\n",
    "partners_data = []\n",
    "\n",
    "# Loop through the first 5 links and scrape details\n",
    "for link in urls:\n",
    "    response = requests.get(link)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    try:\n",
    "        name = soup.select_one(\"h1.company-name\").text\n",
    "    except:\n",
    "        name = \"Not Available\"\n",
    "\n",
    "    try:\n",
    "        corp_partnership_levels_div = soup.select_one(\"div.corp-partnership-levels\")\n",
    "        location = corp_partnership_levels_div.select_one(\".country.small-caps\").text\n",
    "    except:\n",
    "        location = \"Not Available\"\n",
    "\n",
    "    try:\n",
    "        url = soup.select_one('a[rel=\"noopener\"]').get(\"href\")\n",
    "    except:\n",
    "        url = \"Not Available\"\n",
    "\n",
    "    try:\n",
    "        level = corp_partnership_levels_div.select_one(\"p.level\").text\n",
    "    except:\n",
    "        level = \"Not Available\"\n",
    "\n",
    "    # Append scraped data to the list\n",
    "    partners_data.append({\n",
    "        'Partner Name': name,\n",
    "        'Partner Location': location,\n",
    "        'Partner URL': url,\n",
    "        'Partner Level': level\n",
    "    })\n",
    "\n",
    "# Print the scraped data for verification\n",
    "for data in partners_data:\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "23746c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Partner Name': ' Accenture, LLP ', 'Partner Location': 'Not Available', 'Partner URL': 'https://www.accenture.com', 'Partner Level': 'Not Available'}\n",
      "{'Partner Name': ' Capgemini US ', 'Partner Location': 'Not Available', 'Partner URL': 'http://www.capgemini.com', 'Partner Level': 'Not Available'}\n",
      "{'Partner Name': ' Cognizant Technology Solutions ', 'Partner Location': 'Not Available', 'Partner URL': 'http://www.cognizant.com/', 'Partner Level': 'Not Available'}\n",
      "{'Partner Name': ' Infosys Limited ', 'Partner Location': 'Not Available', 'Partner URL': 'https://www.infosys.com', 'Partner Level': 'Not Available'}\n",
      "{'Partner Name': ' LTIMindtree ', 'Partner Location': 'Not Available', 'Partner URL': 'https://www.ltimindtree.com', 'Partner Level': 'Not Available'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Load the previously scraped CSV file\n",
    "df_links = pd.read_csv(\"scraped_links.csv\")\n",
    "urls = df_links[\"Links\"].head(5).tolist()\n",
    "\n",
    "partners_data = []\n",
    "\n",
    "# Loop through the first 5 links and scrape details\n",
    "for link in urls:\n",
    "    response = requests.get(link)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    name = soup.select_one(\"h1.company-name\")\n",
    "    name = name.text if name else \"Not Available\"\n",
    "\n",
    "    corp_partnership_levels_div = soup.select_one(\"div.corp-partnership-levels\")\n",
    "\n",
    "    location = corp_partnership_levels_div.select_one(\".country.small-caps\") if corp_partnership_levels_div else None\n",
    "    location = location.text.strip() if location else \"Not Available\"\n",
    "\n",
    "    partner_url = soup.select_one('a[rel=\"noopener\"]')\n",
    "    url = partner_url.get(\"href\") if partner_url else \"Not Available\"\n",
    "\n",
    "    level = corp_partnership_levels_div.select_one(\"p.level\") if corp_partnership_levels_div else None\n",
    "    level = level.text.strip() if level else \"Not Available\"\n",
    "\n",
    "    # Append scraped data to the list\n",
    "    partners_data.append({\n",
    "        'Partner Name': name,\n",
    "        'Partner Location': location,\n",
    "        'Partner URL': url,\n",
    "        'Partner Level': level\n",
    "    })\n",
    "\n",
    "# Print the scraped data for verification\n",
    "for data in partners_data:\n",
    "    print(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd6c67b",
   "metadata": {},
   "source": [
    "## Fetch All The Urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f2c30e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to partners_data.csv with 244 entries.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Load the previously scraped CSV file\n",
    "df_links = pd.read_csv(\"scraped_links.csv\")\n",
    "urls = df_links[\"Links\"].tolist()  # Removed .head(5) to process all URLs\n",
    "\n",
    "partners_data = []\n",
    "\n",
    "# Loop through all the links and scrape details\n",
    "for link in urls:\n",
    "    response = requests.get(link)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    name = soup.select_one(\"h1.company-name\")\n",
    "    name = name.text if name else \"Not Available\"\n",
    "\n",
    "    partner_url = soup.select_one('a[rel=\"noopener\"]')\n",
    "    url = partner_url.get(\"href\") if partner_url else \"Not Available\"\n",
    "\n",
    "    # Append scraped data to the list\n",
    "    partners_data.append({\n",
    "        'Partner Name': name,\n",
    "        'Partner URL': url,\n",
    "    })\n",
    "\n",
    "# Convert the scraped data into a DataFrame\n",
    "df_partners = pd.DataFrame(partners_data)\n",
    "\n",
    "# Save the data to a new CSV file\n",
    "df_partners.to_csv(\"partners_data.csv\", index=False)\n",
    "\n",
    "print(f\"Data saved to partners_data.csv with {len(df_partners)} entries.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d81745b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "mynev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
